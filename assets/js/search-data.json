{
  
    
        "post0": {
            "title": "Fast Parallel Cheminformatics Workflows With Dask",
            "content": "In this post we&#39;ll take a look at how we can use Dask to parallelize and speed up Cheminformatics workflows with a couple of lines of code. As an example we&#39;ll write a scritpt to calculate the properties from Lipinski&#39;s Rule of 5. Just for fun, we&#39;ll also add in the number of rotatable bonds. We&#39;ll start by implementing a simple serial version, then modify that to run in parallel with Dask. . Single Processor Version . First we&#39;ll import the necessary Python libraries. . import pandas as pd from rdkit import Chem from rdkit.Chem import Descriptors . First, let&#39;s ensure that that Pandas dataframes are dispalyed with an appropriate number of decimal places. . pd.options.display.float_format = &quot;{:,.2f}&quot;.format . Next, we&#39;ll define a function to calculate the properties . def calc_r5(smiles): mol = Chem.MolFromSmiles(smiles) dummy = -9999.0 res = [dummy, dummy, dummy, dummy, dummy] if mol: res = [Descriptors.MolWt(mol),Descriptors.MolLogP(mol),Descriptors.NumHDonors(mol), Descriptors.NumHAcceptors(mol),Descriptors.NumRotatableBonds(mol)] return res . Now we&#39;ll read a SMILES file into a Pandas dataframe . infile_name = &quot;test.smi&quot; df = pd.read_csv(infile_name,sep=&quot; &quot;,names=[&quot;SMILES&quot;,&quot;Name&quot;]) . Now we can calculate the properties for the SMILES in the dataframe. . df[&#39;R5&#39;] = df.SMILES.apply(calc_r5) . Now we have all of the properties as a list in one column. This isn&#39;t what we want, however, we can use this trick to create new columns from the list. . df[[&quot;MW&quot;,&quot;LogP&quot;,&quot;HBD&quot;,&quot;HBA&quot;,&quot;Rot&quot;]] = df.R5.to_list() . df . SMILES Name R5 MW LogP HBD HBA Rot . 0 Br.CC(NC(C)(C)C)C(=O)c1cccc(Cl)c1 | 675686 | [320.658, 3.877200000000003, 1, 2, 3] | 320.66 | 3.88 | 1 | 2 | 3 | . 1 Br.Cc1ccc(Sc2ccccc2N3CCNCC3)c(C)c1 | 1379657 | [379.3670000000002, 4.442140000000004, 1, 3, 3] | 379.37 | 4.44 | 1 | 3 | 3 | . 2 Br.CN(C)CCCC1(OCc2cc(ccc12)C#N)c3ccc(F)cc3 | 674732 | [405.31100000000004, 4.390880000000004, 0, 3, 5] | 405.31 | 4.39 | 0 | 3 | 5 | . 3 Br.CN1CCC[C@@H]1Cc2c[nH]c3ccc(CCS(=O)(=O)c4ccc... | 674954 | [463.4410000000002, 4.398900000000004, 1, 3, 6] | 463.44 | 4.40 | 1 | 3 | 6 | . 4 Br.COc1ccc2CN(C)CC[C@@]34C=C[C@H](O)C[C@@H]3Oc... | 443255 | [368.2710000000001, 2.4282000000000004, 1, 4, 1] | 368.27 | 2.43 | 1 | 4 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 95 CC(C)CC(N(C)C)C1(CCC1)c2ccc(Cl)cc2 | 358192 | [279.85499999999996, 4.738000000000005, 0, 1, 5] | 279.85 | 4.74 | 0 | 1 | 5 | . 96 CC(C)CC1(CC=C)C(=O)NC(=O)NC1=O | 2524 | [224.26, 0.9609999999999999, 2, 3, 4] | 224.26 | 0.96 | 2 | 3 | 4 | . 97 CC(C)Cc1ccc(cc1)C(C)C(=O)O | 11674 | [206.28499999999997, 3.073200000000001, 1, 1, 4] | 206.28 | 3.07 | 1 | 1 | 4 | . 98 CC(C)CCC[C@@H](C)[C@H]1CC[C@H]2 C(=C C=C/3 C[C... | 125294 | [384.6480000000003, 7.619000000000009, 1, 1, 6] | 384.65 | 7.62 | 1 | 1 | 6 | . 99 CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)O[C@H]... | 17347 | [505.63700000000034, 2.4028, 3, 7, 11] | 505.64 | 2.40 | 3 | 7 | 11 | . 100 rows × 8 columns . At this point, we no longer need the R5 column, so we can get rid of it. . df.drop(&quot;R5&quot;,axis=1,inplace=True) . Finally, we can write the results to csv file. . df.to_csv(&quot;props.csv&quot;,index=False) . Parallel Version . In order to parallelize this calculation with Dask, we need a couple of addtional imports . import dask.dataframe as dd . We also need to convert the Pandas dataframe to a Dask dataframe. In the constructor for the Dask dataframe, we&#39;ll specify an argument &quot;npartitions&quot;, that specifies the number of chunks to divided the dataframe into for the calculation. This seems to be most efficeint when we set &quot;npartitions&quot; to the number of processors. . num_cores = 8 ddf = dd.from_pandas(df,npartitions=num_cores) . Parallelization with Dask requires a function that accepts a dataframe as input. We can define a function that uses the &quot;apply&quot; from the serial version above. . def df_r5(df_in): return df_in.SMILES.apply(calc_r5) . Now we can parallelize our workflow with a single line of code. . df[&quot;R5&quot;] = ddf.map_partitions(df_r5,meta=&#39;float&#39;).compute(scheduler=&#39;processes&#39;) . We can use the same method we described about to split the &quot;R5&quot; column into mulitple columns. . df[[&quot;MW&quot;,&quot;LogP&quot;,&quot;HBD&quot;,&quot;HBA&quot;,&quot;Rot&quot;]] = df.R5.to_list() df.drop(&quot;R5&quot;,axis=1,inplace=True) . Benchmarking . Let&#39;s take a look at how much of a speedup we can get by using Dask. To do this, we&#39;ll take a look at how long it takes to calculate the five properties above for 1 million molecules from ZINC database. The input file is large so I didn&#39;t include it in this post. Here&#39;s the code I used to perform the benchmark. . import time df = pd.read_csv(&quot;zinc_1M.smi&quot;,sep=&quot; &quot;,names=[&quot;SMILES&quot;,&quot;Name&quot;]) runtime_res = [] for num_cores in range(1,9): ddf = dd.from_pandas(df,npartitions=num_cores) start = time.time() df[&quot;R5&quot;] = ddf.map_partitions(df_r5,meta=&#39;float&#39;).compute(scheduler=&#39;processes&#39;) elapsed = time.time()-start df[[&quot;MW&quot;,&quot;LogP&quot;,&quot;HBD&quot;,&quot;HBA&quot;,&quot;Rot&quot;]] = df.R5.to_list() df.drop(&quot;R5&quot;,axis=1,inplace=True) df.to_csv(&quot;props.csv&quot;,index=False,float_format=&quot;%0.2f&quot;) print(f&quot;{len(df)} molecules processed in {elapsed:0.2f} sec on {num_cores} cores&quot;) runtime_res.append([elapsed,num_cores]) . 1000000 molecules processed in 714.70 sec on 1 cores 1000000 molecules processed in 378.56 sec on 2 cores 1000000 molecules processed in 272.38 sec on 3 cores 1000000 molecules processed in 211.11 sec on 4 cores 1000000 molecules processed in 177.00 sec on 5 cores 1000000 molecules processed in 158.07 sec on 6 cores 1000000 molecules processed in 146.98 sec on 7 cores 1000000 molecules processed in 142.83 sec on 8 cores . Make a datafrme so that we can view the results. We&#39;ll add a column with the runtime ratio for n cores to 1 core. . runtime_df = pd.DataFrame(runtime_res,columns=[&quot;Runtime&quot;,&quot;Cores&quot;]) one_core_time = runtime_df.Runtime.values[0] runtime_df[&#39;Ratio&#39;] = one_core_time/runtime_df.Runtime . View the results. 5x speed-up with 8 cores, not too bad for one line of code. . runtime_df . Runtime Cores Ratio . 0 714.70 | 1 | 1.00 | . 1 378.56 | 2 | 1.89 | . 2 272.38 | 3 | 2.62 | . 3 211.11 | 4 | 3.39 | . 4 177.00 | 5 | 4.04 | . 5 158.07 | 6 | 4.52 | . 6 146.98 | 7 | 4.86 | . 7 142.83 | 8 | 5.00 | . Plot the results. . import seaborn as sns sns.set(rc={&#39;figure.figsize&#39;: (10, 10)}) sns.set_style(&#39;whitegrid&#39;) sns.set_context(&#39;talk&#39;) ax = sns.scatterplot(x=&quot;Cores&quot;,y=&quot;Runtime&quot;,data=runtime_df) ax.set(ylabel=&quot;Runtime (sec)&quot;); . Acknowledgments . I&#39;d like to thank Yutong Zhao, Greg Landrum, Peter St. John, and Maciek Wójcikowski for valuable advice. .",
            "url": "https://patwalters.github.io/practicalcheminformatics/jupyter/2021/03/28/dask-cheminformatics.html",
            "relUrl": "/jupyter/2021/03/28/dask-cheminformatics.html",
            "date": " • Mar 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I do Cheminformatics, sometimes I write about it. .",
          "url": "https://patwalters.github.io/practicalcheminformatics/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patwalters.github.io/practicalcheminformatics/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}